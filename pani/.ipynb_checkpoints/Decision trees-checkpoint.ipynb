{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import math\n",
    "\n",
    "mat = scipy.io.loadmat('../data/cleandata_students.mat')\n",
    "x = mat['x'] # N examples each having 45 action unit features\n",
    "N = len(x)   # Number of examples\n",
    "y = mat['y'] # correct labels of N examples\n",
    "attributes = [\"AU\" + str(i) for i in range(1, 46)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, kids=[], Class=None, op=None):\n",
    "        self.kids = kids\n",
    "        self.Class = Class # this might be a problem\n",
    "        self.op = op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p, n):\n",
    "    if (p == 0 or n == 0):\n",
    "        return 0\n",
    "    \n",
    "    k0 = p / (p + n)\n",
    "    k1 = n / (p + n)\n",
    "    \n",
    "    return - k0 * math.log2(k0) - k1 * math.log2(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-73-1c62427a9a83>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-1c62427a9a83>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return attributes[0]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def all_elements_equal(l):\n",
    "    for i in range(len(l)):\n",
    "        if l[i] != l[0]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def majority(l):\n",
    "    total = 0\n",
    "    for i in range(len(l)):\n",
    "        if l[i] == 0:\n",
    "            total -= 1\n",
    "        else:\n",
    "            total += 1\n",
    "    if total < 0:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def choose_best_decision_attribute(examples, attributes, binary_attributes):\n",
    "    # Choose attribute with highest information gain\n",
    "    '''\n",
    "    find p and n\n",
    "    IG = entropy of inital examples\n",
    "    for each attribute\n",
    "        get subset of the data and find number of positive and negative examples\n",
    "        IG -= \n",
    "    '''\n",
    "    highest = 0  # ?\n",
    "    total = len(binary_attributes)\n",
    "    p = len([x for x in binary_attributes if x == 1])\n",
    "    n = total - p\n",
    "    \n",
    "    IG = entropy(p, n)\n",
    "    for attribute in attributes:\n",
    "        p0 = \n",
    "        n0 =\n",
    "        IG -= () * entropy(p0, n0)\n",
    "    \n",
    "        p1 =\n",
    "        n1 = \n",
    "        IG -= () * entropy(n1, n1)\n",
    "        \n",
    "    return highest\n",
    "\n",
    "def decision_tree_learning(examples, attributes, binary_targets):\n",
    "    if all_elements_equal(binary_target):\n",
    "        leaf = Node(Class=binary_target[0])\n",
    "        return leaf\n",
    "    elif len(attributes) == 0:\n",
    "        leaf = Node(Class=majority(binary_target))\n",
    "        return leaf\n",
    "\n",
    "    best_attribute = choose_best_decision_attribute(examples, attributes, binary_targets)    \n",
    "    root = Node(op=best_attribute)\n",
    "    for value in range(0, 2): # for all possible values of attribute\n",
    "        root.kids.append(Node())\n",
    "        \n",
    "        l = [x for x in zip(examples, binary_targets) if x[0][best_attribute] == value]\n",
    "        examples_i, binary_targets_i = zip(*l)\n",
    "        \n",
    "        if len(examples_i) == 0:\n",
    "            return Node(Class=majority(binary_targets))\n",
    "        else:\n",
    "            next_attributes = [x for x in attribute if x != best_attribute]\n",
    "            return decision_tree_learning(examples_i, next_attributes, binary_targets_i)\n",
    "\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct all trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['woohoo'], ['woohoo'], ['woohoo'], ['woohoo'], ['woohoo'], ['woohoo']]\n"
     ]
    }
   ],
   "source": [
    "# Emotions have labels 1 .. 6\n",
    "trees = []\n",
    "for i in range(1, 7):\n",
    "    binary_targets = [i in example for example in y]\n",
    "    trees.append(decision_tree_learning(x, attributes, binary_targets))\n",
    "    \n",
    "# We represent trees by their root node\n",
    "print(trees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
